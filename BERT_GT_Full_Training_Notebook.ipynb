{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT-GT for BioRED Relation Extraction\n",
    "\n",
    "This notebook implements **BERT-GT (BERT with Graph Transformer)** - the official model from the BioRED paper.\n",
    "\n",
    "## What is BERT-GT?\n",
    "\n",
    "BERT-GT improves relation extraction by:\n",
    "- **Graph Transformer layers** that model entity interactions\n",
    "- **Document-level reasoning** for cross-sentence relations\n",
    "- **Entity-aware attention** focusing on entity pairs\n",
    "\n",
    "## Performance\n",
    "\n",
    "| Model | Entity Pair F1 | + Type F1 |\n",
    "|-------|----------------|------------|\n",
    "| BioBERT | ~65% | ~52% |\n",
    "| **BERT-GT** | **~73%** | **~59%** |\n",
    "\n",
    "**Reference**: https://github.com/ncbi/bert_gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (4.29.2)\n",
      "Requirement already satisfied: torch in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: scikit-learn in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: tqdm in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (4.65.0)\n",
      "Requirement already satisfied: filelock in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/segr11/.local/lib/python3.11/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/segr11/.local/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/segr11/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/segr11/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/segr11/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (1.2.0)\n",
      "Requirement already satisfied: sympy in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/segr11/.local/lib/python3.11/site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install transformers torch scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Graph Transformer Layer\n",
    "\n",
    "The core innovation of BERT-GT: treats entities as nodes in a graph and uses graph attention to model their relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GraphTransformerLayer defined\n"
     ]
    }
   ],
   "source": [
    "class GraphTransformerLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Transformer Layer for modeling entity interactions.\n",
    "    \n",
    "    This layer treats entities as nodes in a graph and uses\n",
    "    graph attention to model their relationships.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, num_heads=4, dropout=0.1):\n",
    "        super(GraphTransformerLayer, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        \n",
    "        assert hidden_size % num_heads == 0, \"hidden_size must be divisible by num_heads\"\n",
    "        \n",
    "        # Multi-head attention for graph\n",
    "        self.query = nn.Linear(hidden_size, hidden_size)\n",
    "        self.key = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size * 4, hidden_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, entity_reprs, adjacency_matrix=None):\n",
    "        \"\"\"\n",
    "        Apply graph transformer layer.\n",
    "        \n",
    "        Args:\n",
    "            entity_reprs: [batch_size, num_entities, hidden_size]\n",
    "            adjacency_matrix: [batch_size, num_entities, num_entities] (optional)\n",
    "        \n",
    "        Returns:\n",
    "            Updated entity representations [batch_size, num_entities, hidden_size]\n",
    "        \"\"\"\n",
    "        batch_size, num_entities, hidden_size = entity_reprs.size()\n",
    "        \n",
    "        # Multi-head self-attention on graph\n",
    "        Q = self.query(entity_reprs).view(batch_size, num_entities, self.num_heads, self.head_dim)\n",
    "        K = self.key(entity_reprs).view(batch_size, num_entities, self.num_heads, self.head_dim)\n",
    "        V = self.value(entity_reprs).view(batch_size, num_entities, self.num_heads, self.head_dim)\n",
    "        \n",
    "        Q = Q.transpose(1, 2)  # [batch_size, num_heads, num_entities, head_dim]\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Apply adjacency matrix if provided (mask non-neighbors)\n",
    "        if adjacency_matrix is not None:\n",
    "            adjacency_matrix = adjacency_matrix.unsqueeze(1)  # [batch_size, 1, num_entities, num_entities]\n",
    "            attention_scores = attention_scores.masked_fill(adjacency_matrix == 0, -1e9)\n",
    "        \n",
    "        # Attention weights\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        context = torch.matmul(attention_probs, V)  # [batch_size, num_heads, num_entities, head_dim]\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, num_entities, hidden_size)\n",
    "        \n",
    "        # Output projection + residual\n",
    "        output = self.out_proj(context)\n",
    "        output = self.dropout(output)\n",
    "        entity_reprs = self.layer_norm1(entity_reprs + output)\n",
    "        \n",
    "        # Feed-forward + residual\n",
    "        ffn_output = self.ffn(entity_reprs)\n",
    "        entity_reprs = self.layer_norm2(entity_reprs + ffn_output)\n",
    "        \n",
    "        return entity_reprs\n",
    "\n",
    "print(\"✓ GraphTransformerLayer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BERT-GT Model\n",
    "\n",
    "Complete BERT-GT architecture:\n",
    "1. BERT encoder for token representations\n",
    "2. Entity extraction and projection\n",
    "3. Graph Transformer layers\n",
    "4. Entity pair classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ BERTGTModel defined\n"
     ]
    }
   ],
   "source": [
    "class BERTGTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete BERT-GT model for document-level relation extraction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name='dmis-lab/biobert-v1.1',\n",
    "        num_relations=4,\n",
    "        num_graph_layers=2,\n",
    "        num_attention_heads=4,\n",
    "        max_entities=20,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super(BERTGTModel, self).__init__()\n",
    "        \n",
    "        # BioBERT encoder\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        self.max_entities = max_entities\n",
    "        \n",
    "        # Entity representation layer\n",
    "        self.entity_projection = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "        # Graph Transformer layers\n",
    "        self.graph_layers = nn.ModuleList([\n",
    "            GraphTransformerLayer(\n",
    "                self.hidden_size,\n",
    "                num_heads=num_attention_heads,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(num_graph_layers)\n",
    "        ])\n",
    "        \n",
    "        # Entity pair combination: [e1, e2, e1*e2]\n",
    "        self.pair_projection = nn.Linear(self.hidden_size * 3, self.hidden_size)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Relation classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.hidden_size, num_relations)\n",
    "        )\n",
    "    \n",
    "    def extract_entity_representations(self, sequence_output, entity_positions, batch_size):\n",
    "        \"\"\"\n",
    "        Extract entity representations from BERT output using average pooling.\n",
    "        \"\"\"\n",
    "        all_entity_reprs = []\n",
    "        entity_masks = []\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            batch_entities = entity_positions[b]\n",
    "            entity_reprs = []\n",
    "            \n",
    "            for entity in batch_entities[:self.max_entities]:\n",
    "                start = entity['start']\n",
    "                end = entity['end'] + 1  # Inclusive end\n",
    "                \n",
    "                # Average pooling over entity tokens\n",
    "                entity_tokens = sequence_output[b, start:end, :]\n",
    "                if entity_tokens.size(0) > 0:\n",
    "                    entity_repr = entity_tokens.mean(dim=0)\n",
    "                else:\n",
    "                    entity_repr = torch.zeros(self.hidden_size, device=sequence_output.device)\n",
    "                \n",
    "                entity_reprs.append(entity_repr)\n",
    "            \n",
    "            # Pad to max_entities\n",
    "            num_entities = len(entity_reprs)\n",
    "            mask = [1] * num_entities + [0] * (self.max_entities - num_entities)\n",
    "            \n",
    "            while len(entity_reprs) < self.max_entities:\n",
    "                entity_reprs.append(torch.zeros(self.hidden_size, device=sequence_output.device))\n",
    "            \n",
    "            all_entity_reprs.append(torch.stack(entity_reprs[:self.max_entities]))\n",
    "            entity_masks.append(mask)\n",
    "        \n",
    "        entity_reprs = torch.stack(all_entity_reprs)  # [batch_size, max_entities, hidden_size]\n",
    "        entity_masks = torch.tensor(entity_masks, device=sequence_output.device)\n",
    "        \n",
    "        return entity_reprs, entity_masks\n",
    "    \n",
    "    def build_adjacency_matrix(self, entity_positions, batch_size):\n",
    "        \"\"\"\n",
    "        Build adjacency matrix for entity graph.\n",
    "        Uses fully connected graph (all entities can attend to each other).\n",
    "        \"\"\"\n",
    "        adjacency_matrices = []\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            num_entities = min(len(entity_positions[b]), self.max_entities)\n",
    "            \n",
    "            # Fully connected graph\n",
    "            adj_matrix = torch.zeros(self.max_entities, self.max_entities)\n",
    "            adj_matrix[:num_entities, :num_entities] = 1\n",
    "            \n",
    "            adjacency_matrices.append(adj_matrix)\n",
    "        \n",
    "        return torch.stack(adjacency_matrices).to(self.bert.device)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, entity1_start, entity1_end, \n",
    "                entity2_start, entity2_end, entities, labels=None):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \"\"\"\n",
    "        batch_size = input_ids.size(0)\n",
    "        \n",
    "        # Get BERT representations\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        sequence_output = outputs.last_hidden_state  # [batch_size, seq_length, hidden_size]\n",
    "        \n",
    "        # Extract entity representations\n",
    "        entity_reprs, entity_mask = self.extract_entity_representations(\n",
    "            sequence_output, entities, batch_size\n",
    "        )\n",
    "        \n",
    "        # Project entity representations\n",
    "        entity_reprs = self.entity_projection(entity_reprs)\n",
    "        entity_reprs = self.dropout(entity_reprs)\n",
    "        \n",
    "        # Build adjacency matrix\n",
    "        adjacency_matrix = self.build_adjacency_matrix(entities, batch_size)\n",
    "        \n",
    "        # Apply Graph Transformer layers\n",
    "        for graph_layer in self.graph_layers:\n",
    "            entity_reprs = graph_layer(entity_reprs, adjacency_matrix)\n",
    "        \n",
    "        # Get target entity pair representations\n",
    "        pair_reprs = []\n",
    "        for b in range(batch_size):\n",
    "            # Find target entities\n",
    "            target_ent1_start = entity1_start[b]\n",
    "            target_ent2_start = entity2_start[b]\n",
    "            \n",
    "            ent1_idx = None\n",
    "            ent2_idx = None\n",
    "            for i, entity in enumerate(entities[b][:self.max_entities]):\n",
    "                if entity['start'] == target_ent1_start:\n",
    "                    ent1_idx = i\n",
    "                if entity['start'] == target_ent2_start:\n",
    "                    ent2_idx = i\n",
    "            \n",
    "            # Get representations\n",
    "            if ent1_idx is not None and ent2_idx is not None:\n",
    "                e1_repr = entity_reprs[b, ent1_idx]\n",
    "                e2_repr = entity_reprs[b, ent2_idx]\n",
    "            else:\n",
    "                e1_repr = torch.zeros(self.hidden_size, device=entity_reprs.device)\n",
    "                e2_repr = torch.zeros(self.hidden_size, device=entity_reprs.device)\n",
    "            \n",
    "            # Combine: [e1, e2, e1*e2]\n",
    "            pair_repr = torch.cat([e1_repr, e2_repr, e1_repr * e2_repr], dim=0)\n",
    "            pair_reprs.append(pair_repr)\n",
    "        \n",
    "        pair_reprs = torch.stack(pair_reprs)  # [batch_size, hidden_size * 3]\n",
    "        \n",
    "        # Project and classify\n",
    "        pair_repr = self.pair_projection(pair_reprs)\n",
    "        pair_repr = self.dropout(pair_repr)\n",
    "        logits = self.classifier(pair_repr)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "        \n",
    "        return {'loss': loss, 'logits': logits} if loss is not None else {'logits': logits}\n",
    "\n",
    "print(\"✓ BERTGTModel defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Converter\n",
    "\n",
    "Convert BioRED PubTator format to BERT-GT format with entity positions and graph structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Fixed BioREDToBERTGTConverter defined\n"
     ]
    }
   ],
   "source": [
    "class BioREDToBERTGTConverter:\n",
    "    \"\"\"\n",
    "    Converts BioRED documents to BERT-GT format.\n",
    "    FIXED VERSION - handles various entity type names.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, max_seq_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self._debug_mode = True  # Set to False after testing\n",
    "    \n",
    "    def convert_documents(self, documents):\n",
    "        \"\"\"\n",
    "        Convert BioRED documents to BERT-GT format.\n",
    "        \"\"\"\n",
    "        bert_gt_examples = []\n",
    "        \n",
    "        for doc_idx, doc in enumerate(tqdm(documents, desc=\"Converting to BERT-GT format\")):\n",
    "            # Get text and entities\n",
    "            text = doc['text']\n",
    "            entities = doc['entities']\n",
    "            relations = doc['relations']\n",
    "            \n",
    "            if self._debug_mode and doc_idx == 0:\n",
    "                print(f\"\\n  DEBUG: First document\")\n",
    "                print(f\"    Text: '{text[:100]}...'\")\n",
    "                print(f\"    Entities: {len(entities)}\")\n",
    "                print(f\"    Relations: {len(relations)}\")\n",
    "            \n",
    "            # Tokenize\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                max_length=self.max_seq_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_offsets_mapping=True\n",
    "            )\n",
    "            \n",
    "            # Map entities to token positions\n",
    "            token_to_char = encoding['offset_mapping']\n",
    "            entity_token_positions = []\n",
    "            \n",
    "            for ent in entities:\n",
    "                char_start = ent['start']\n",
    "                char_end = ent['end']\n",
    "                \n",
    "                # Find token positions\n",
    "                token_start = None\n",
    "                token_end = None\n",
    "                \n",
    "                for i, (t_start, t_end) in enumerate(token_to_char):\n",
    "                    if t_start <= char_start < t_end and token_start is None:\n",
    "                        token_start = i\n",
    "                    if t_start < char_end <= t_end:\n",
    "                        token_end = i\n",
    "                        break\n",
    "                \n",
    "                # Also accept if token overlaps with entity\n",
    "                if token_start is None or token_end is None:\n",
    "                    for i, (t_start, t_end) in enumerate(token_to_char):\n",
    "                        # Check overlap\n",
    "                        if t_start <= char_start < t_end:\n",
    "                            token_start = i\n",
    "                        if t_start <= char_end <= t_end or (t_start < char_end and char_end <= t_end):\n",
    "                            token_end = i\n",
    "                            break\n",
    "                \n",
    "                if token_start is not None and token_end is not None:\n",
    "                    entity_token_positions.append({\n",
    "                        'id': ent['id'],\n",
    "                        'type': ent['type'],\n",
    "                        'start': token_start,\n",
    "                        'end': token_end,\n",
    "                        'text': ent['text']\n",
    "                    })\n",
    "            \n",
    "            if self._debug_mode and doc_idx == 0:\n",
    "                print(f\"    Mapped entities: {len(entity_token_positions)}/{len(entities)}\")\n",
    "                if len(entity_token_positions) > 0:\n",
    "                    print(f\"    First mapped entity: {entity_token_positions[0]}\")\n",
    "            \n",
    "            # Create entity pairs\n",
    "            pairs_created = 0\n",
    "            for i, ent1 in enumerate(entity_token_positions):\n",
    "                for ent2 in entity_token_positions[i+1:]:\n",
    "                    # Check if valid pair\n",
    "                    if not self._is_valid_pair(ent1['type'], ent2['type']):\n",
    "                        continue\n",
    "                    \n",
    "                    # Find relation label\n",
    "                    relation_label = self._find_relation(ent1['id'], ent2['id'], relations)\n",
    "                    \n",
    "                    # Create example\n",
    "                    example = {\n",
    "                        'input_ids': encoding['input_ids'],\n",
    "                        'attention_mask': encoding['attention_mask'],\n",
    "                        'entity1_start': ent1['start'],\n",
    "                        'entity1_end': ent1['end'],\n",
    "                        'entity1_type': ent1['type'],\n",
    "                        'entity2_start': ent2['start'],\n",
    "                        'entity2_end': ent2['end'],\n",
    "                        'entity2_type': ent2['type'],\n",
    "                        'entities': entity_token_positions,\n",
    "                        'label': relation_label,\n",
    "                        'pmid': doc['pmid']\n",
    "                    }\n",
    "                    \n",
    "                    bert_gt_examples.append(example)\n",
    "                    pairs_created += 1\n",
    "            \n",
    "            if self._debug_mode and doc_idx == 0:\n",
    "                print(f\"    Pairs created: {pairs_created}\")\n",
    "        \n",
    "        # Turn off debug after first document\n",
    "        self._debug_mode = False\n",
    "        \n",
    "        return bert_gt_examples\n",
    "    \n",
    "    def _is_valid_pair(self, type1, type2):\n",
    "        \"\"\"Check if entity pair is valid for RE.\"\"\"\n",
    "        \n",
    "        # Normalize entity types\n",
    "        def normalize_type(t):\n",
    "            t = t.lower()\n",
    "            # Map various type names to standard names\n",
    "            if 'gene' in t or 'protein' in t:\n",
    "                return 'gene'\n",
    "            if 'disease' in t or 'phenotype' in t:\n",
    "                return 'disease'\n",
    "            if 'variant' in t or 'mutation' in t or 'snp' in t:\n",
    "                return 'variant'\n",
    "            if 'chemical' in t or 'drug' in t:\n",
    "                return 'chemical'\n",
    "            return t\n",
    "        \n",
    "        t1 = normalize_type(type1)\n",
    "        t2 = normalize_type(type2)\n",
    "        \n",
    "        # Valid pairs\n",
    "        valid_pairs = [\n",
    "            ('disease', 'gene'),\n",
    "            ('gene', 'disease'),\n",
    "            ('disease', 'variant'),\n",
    "            ('variant', 'disease'),\n",
    "            ('gene', 'variant'),\n",
    "            ('variant', 'gene'),\n",
    "            ('chemical', 'disease'),\n",
    "            ('disease', 'chemical'),\n",
    "            ('chemical', 'gene'),\n",
    "            ('gene', 'chemical'),\n",
    "        ]\n",
    "        \n",
    "        return (t1, t2) in valid_pairs\n",
    "    \n",
    "    def _find_relation(self, id1, id2, relations):\n",
    "        \"\"\"Find relation between entities.\"\"\"\n",
    "        for rel in relations:\n",
    "            # Handle different relation formats\n",
    "            arg1_id = rel['arg1'].split(':')[-1] if ':' in rel['arg1'] else rel['arg1']\n",
    "            arg2_id = rel['arg2'].split(':')[-1] if ':' in rel['arg2'] else rel['arg2']\n",
    "            \n",
    "            if (arg1_id == id1 and arg2_id == id2) or (arg1_id == id2 and arg2_id == id1):\n",
    "                return rel['type']\n",
    "        \n",
    "        return 'No_Relation'\n",
    "\n",
    "print(\"✓ Fixed BioREDToBERTGTConverter defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset and DataLoader\n",
    "\n",
    "PyTorch Dataset for BERT-GT with custom collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ BERTGTDataset and collate_fn defined\n"
     ]
    }
   ],
   "source": [
    "class BERTGTDataset(Dataset):\n",
    "    \"\"\"Dataset for BERT-GT model.\"\"\"\n",
    "    \n",
    "    def __init__(self, examples, relation2id):\n",
    "        self.examples = examples\n",
    "        self.relation2id = relation2id\n",
    "        self.id2relation = {v: k for k, v in relation2id.items()}\n",
    "        self.num_relations = len(relation2id)\n",
    "        self.relation_types = list(relation2id.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        \n",
    "        # Convert relation to ID\n",
    "        label = self.relation2id.get(example['label'], self.relation2id['No_Relation'])\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(example['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(example['attention_mask'], dtype=torch.long),\n",
    "            'entity1_start': example['entity1_start'],\n",
    "            'entity1_end': example['entity1_end'],\n",
    "            'entity2_start': example['entity2_start'],\n",
    "            'entity2_end': example['entity2_end'],\n",
    "            'entities': example['entities'],\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def bert_gt_collate_fn(batch):\n",
    "    \"\"\"Custom collate function for BERT-GT.\"\"\"\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.stack([item['label'] for item in batch])\n",
    "    \n",
    "    entity1_start = [item['entity1_start'] for item in batch]\n",
    "    entity1_end = [item['entity1_end'] for item in batch]\n",
    "    entity2_start = [item['entity2_start'] for item in batch]\n",
    "    entity2_end = [item['entity2_end'] for item in batch]\n",
    "    entities = [item['entities'] for item in batch]\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'entity1_start': entity1_start,\n",
    "        'entity1_end': entity1_end,\n",
    "        'entity2_start': entity2_start,\n",
    "        'entity2_end': entity2_end,\n",
    "        'entities': entities,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "print(\"✓ BERTGTDataset and collate_fn defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training and evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "def train_bert_gt(model, train_loader, val_loader, num_epochs=10, learning_rate=1e-5, device='cuda'):\n",
    "    \"\"\"\n",
    "    Train BERT-GT model.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Scheduler\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_f1 = 0\n",
    "    training_stats = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc='Training')\n",
    "        for batch in train_pbar:\n",
    "            # Move to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                entity1_start=batch['entity1_start'],\n",
    "                entity1_end=batch['entity1_end'],\n",
    "                entity2_start=batch['entity2_start'],\n",
    "                entity2_end=batch['entity2_end'],\n",
    "                entities=batch['entities'],\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs['loss']\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        print(f\"\\nAverage training loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        val_metrics = evaluate_bert_gt(model, val_loader, device)\n",
    "        \n",
    "        print(f\"Validation F1: {val_metrics['f1']:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['f1'] > best_val_f1:\n",
    "            best_val_f1 = val_metrics['f1']\n",
    "            torch.save(model.state_dict(), 'best_bert_gt_model.pt')\n",
    "            print(f\"✓ Saved best model with F1: {best_val_f1:.4f}\")\n",
    "        \n",
    "        training_stats.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_f1': val_metrics['f1'],\n",
    "            'val_accuracy': val_metrics['accuracy']\n",
    "        })\n",
    "    \n",
    "    return training_stats\n",
    "\n",
    "\n",
    "def evaluate_bert_gt(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate BERT-GT model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc='Evaluating'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                entity1_start=batch['entity1_start'],\n",
    "                entity1_end=batch['entity1_end'],\n",
    "                entity2_start=batch['entity2_start'],\n",
    "                entity2_end=batch['entity2_end'],\n",
    "                entities=batch['entities']\n",
    "            )\n",
    "            \n",
    "            logits = outputs['logits']\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return {'f1': f1, 'accuracy': accuracy}\n",
    "\n",
    "print(\"✓ Training and evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load and Prepare Data\n",
    "\n",
    "Now we'll load the BioRED data and convert it to BERT-GT format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  TESTING MODE\n",
      "\n",
      "Configuration:\n",
      "  Max documents: 100\n",
      "  Epochs: 3\n",
      "  Batch size: 4\n",
      "  Learning rate: 1e-05\n",
      "  Graph layers: 2\n",
      "  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "# Set this to True for testing, False for full training\n",
    "TESTING = False\n",
    "\n",
    "# File paths (UPDATE THESE!)\n",
    "TRAIN_DATA_PATH = './data/Train.PubTator'\n",
    "DEV_DATA_PATH = './data/Dev.PubTator'\n",
    "TEST_DATA_PATH = './data/Test.PubTator'\n",
    "\n",
    "# Model settings\n",
    "MODEL_NAME = 'dmis-lab/biobert-v1.1'\n",
    "MAX_LENGTH = 512\n",
    "NUM_GRAPH_LAYERS = 2  # Number of Graph Transformer layers\n",
    "NUM_ATTENTION_HEADS = 4\n",
    "MAX_ENTITIES = 20\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# Training settings\n",
    "if TESTING:\n",
    "    print(\"⚠️  TESTING MODE\")\n",
    "    MAX_DOCS = 100  # Load 100 docs for testing\n",
    "    NUM_EPOCHS = 3\n",
    "    BATCH_SIZE = 4\n",
    "    LEARNING_RATE = 1e-5\n",
    "else:\n",
    "    print(\"✓ FULL TRAINING MODE\")\n",
    "    MAX_DOCS = None  # Load all documents\n",
    "    NUM_EPOCHS = 30\n",
    "    BATCH_SIZE = 8\n",
    "    LEARNING_RATE = 1e-5\n",
    "\n",
    "# Relation types\n",
    "RELATION_TYPES = {\n",
    "    'Positive_Correlation': 0,\n",
    "    'Negative_Correlation': 1,\n",
    "    'Association': 2,\n",
    "    'No_Relation': 3\n",
    "}\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Max documents: {MAX_DOCS or 'ALL'}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Graph layers: {NUM_GRAPH_LAYERS}\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BioRED data...\n",
      "\n",
      "\n",
      "Loading: Train.PubTator\n",
      "  Loaded 100 documents...\r\n",
      "✓ Reached max_documents limit (100)\n",
      "\n",
      "Loading: Dev.PubTator\n",
      "\n",
      "✓ Reached max_documents limit (50)\n",
      "\n",
      "Loading: Test.PubTator\n",
      "\n",
      "✓ Reached max_documents limit (50)\n",
      "\n",
      "✓ Loaded:\n",
      "  Train: 100 documents\n",
      "  Dev: 50 documents\n",
      "  Test: 50 documents\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "#              LOAD BIORED DATA \n",
    "# ============================================\n",
    "\n",
    "class BioREDDataLoader:\n",
    "    \"\"\"\n",
    "    Loads and parses BioRED dataset in PubTator format.\n",
    "    \n",
    "    PubTator format:\n",
    "    - First line: PMID|t|Title\n",
    "    - Second line: PMID|a|Abstract\n",
    "    - Following lines: PMID\\tstart\\tend\\ttext\\ttype\\tid\n",
    "    - Relation lines: PMID\\trelation_type\\tArg1:entity_id\\tArg2:entity_id\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to .pubtator file, directory containing .pubtator files, or list of paths\n",
    "        max_documents: Maximum number of documents to load (None = load all)\n",
    "        skip_documents: Number of documents to skip from the beginning\n",
    "        verbose: Print loading progress\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_path, max_documents=None, skip_documents=0, verbose=True):\n",
    "        self.file_path = file_path\n",
    "        self.max_documents = max_documents\n",
    "        self.skip_documents = skip_documents\n",
    "        self.verbose = verbose\n",
    "        self.documents = []\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and parse BioRED data from file(s).\"\"\"\n",
    "        # Determine if file_path is a file, directory, or list\n",
    "        if isinstance(self.file_path, list):\n",
    "            # List of file paths\n",
    "            file_paths = self.file_path\n",
    "            if self.verbose:\n",
    "                print(f\"Loading from {len(file_paths)} file(s)\")\n",
    "        elif os.path.isdir(self.file_path):\n",
    "            # Directory - get all .pubtator files\n",
    "            file_paths = sorted([\n",
    "                os.path.join(self.file_path, f) \n",
    "                for f in os.listdir(self.file_path) \n",
    "                if f.endswith('.pubtator')\n",
    "            ])\n",
    "            if self.verbose:\n",
    "                print(f\"Found {len(file_paths)} .pubtator file(s) in directory\")\n",
    "        else:\n",
    "            # Single file\n",
    "            file_paths = [self.file_path]\n",
    "        \n",
    "        if not file_paths:\n",
    "            print(\"Warning: No .pubtator files found!\")\n",
    "            return self.documents\n",
    "        \n",
    "        # Load documents from all files\n",
    "        docs_processed = 0\n",
    "        \n",
    "        for file_path in file_paths:\n",
    "            if self.verbose:\n",
    "                print(f\"\\nLoading: {os.path.basename(file_path)}\")\n",
    "            \n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read().strip()\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Error: File not found - {file_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Split by empty lines (documents are separated by empty lines)\n",
    "            doc_blocks = content.split('\\n\\n')\n",
    "            \n",
    "            for block in doc_blocks:\n",
    "                # Check if we should skip this document\n",
    "                if docs_processed < self.skip_documents:\n",
    "                    docs_processed += 1\n",
    "                    continue\n",
    "                \n",
    "                # Check if we've reached the maximum\n",
    "                if self.max_documents is not None and len(self.documents) >= self.max_documents:\n",
    "                    if self.verbose:\n",
    "                        print(f\"\\n✓ Reached max_documents limit ({self.max_documents})\")\n",
    "                    return self.documents\n",
    "                \n",
    "                if not block.strip():\n",
    "                    continue\n",
    "                    \n",
    "                doc = self._parse_document(block)\n",
    "                if doc:\n",
    "                    self.documents.append(doc)\n",
    "                    docs_processed += 1\n",
    "                    \n",
    "                    # Print progress every 100 documents\n",
    "                    if self.verbose and len(self.documents) % 100 == 0:\n",
    "                        print(f\"  Loaded {len(self.documents)} documents...\", end='\\r')\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\\n✓ Loaded {len(self.documents)} documents total\")\n",
    "            if self.skip_documents > 0:\n",
    "                print(f\"  (Skipped first {self.skip_documents} documents)\")\n",
    "        \n",
    "        return self.documents\n",
    "    \n",
    "    def _parse_document(self, block):\n",
    "        \"\"\"Parse a single document block.\"\"\"\n",
    "        lines = block.strip().split('\\n')\n",
    "        if len(lines) < 2:\n",
    "            return None\n",
    "        \n",
    "        # Parse title and abstract\n",
    "        title_parts = lines[0].split('|t|')\n",
    "        abstract_parts = lines[1].split('|a|')\n",
    "        \n",
    "        if len(title_parts) < 2 or len(abstract_parts) < 2:\n",
    "            return None\n",
    "        \n",
    "        pmid = title_parts[0]\n",
    "        title = title_parts[1] if len(title_parts) > 1 else \"\"\n",
    "        abstract = abstract_parts[1] if len(abstract_parts) > 1 else \"\"\n",
    "        \n",
    "        text = title + \" \" + abstract\n",
    "        \n",
    "        # Parse entities and relations\n",
    "        entities = []\n",
    "        relations = []\n",
    "        \n",
    "        for line in lines[2:]:\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) >= 6:  # Entity annotation\n",
    "                entity = {\n",
    "                    'pmid': parts[0],\n",
    "                    'start': int(parts[1]),\n",
    "                    'end': int(parts[2]),\n",
    "                    'text': parts[3],\n",
    "                    'type': parts[4],\n",
    "                    'id': parts[5]\n",
    "                }\n",
    "                entities.append(entity)\n",
    "            elif len(parts) >= 4 and 'CID' not in parts[1]:  # Relation annotation\n",
    "                relation = {\n",
    "                    'pmid': parts[0],\n",
    "                    'type': parts[1],\n",
    "                    'arg1': parts[2],\n",
    "                    'arg2': parts[3]\n",
    "                }\n",
    "                relations.append(relation)\n",
    "        \n",
    "        return {\n",
    "            'pmid': pmid,\n",
    "            'title': title,\n",
    "            'abstract': abstract,\n",
    "            'text': text,\n",
    "            'entities': entities,\n",
    "            'relations': relations\n",
    "        }\n",
    "\n",
    "print(\"Loading BioRED data...\\n\")\n",
    "\n",
    "train_loader_data = BioREDDataLoader(TRAIN_DATA_PATH, max_documents=MAX_DOCS)\n",
    "dev_loader_data = BioREDDataLoader(DEV_DATA_PATH, max_documents=MAX_DOCS//2 if MAX_DOCS else None)\n",
    "test_loader_data = BioREDDataLoader(TEST_DATA_PATH, max_documents=MAX_DOCS//2 if MAX_DOCS else None)\n",
    "\n",
    "train_docs = train_loader_data.load_data()\n",
    "dev_docs = dev_loader_data.load_data()\n",
    "test_docs = test_loader_data.load_data()\n",
    "\n",
    "print(f\"\\n✓ Loaded:\")\n",
    "print(f\"  Train: {len(train_docs)} documents\")\n",
    "print(f\"  Dev: {len(dev_docs)} documents\")\n",
    "print(f\"  Test: {len(test_docs)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DIAGNOSTIC: Analyzing Loaded Documents\n",
      "============================================================\n",
      "\n",
      "TRAIN Documents:\n",
      "  Total documents: 100\n",
      "\n",
      "  First document (PMID: 10491763):\n",
      "    Title: Hepatocyte nuclear factor-6: associations between genetic variability and type I...\n",
      "    Text length: 1797 chars\n",
      "    Entities: 32\n",
      "    Relations: 3\n",
      "\n",
      "  Entity Statistics:\n",
      "    Documents with entities: 100/100\n",
      "    Total entities: 3343\n",
      "    Entity types:\n",
      "      CellLine: 24\n",
      "      ChemicalEntity: 485\n",
      "      DiseaseOrPhenotypicFeature: 997\n",
      "      GeneOrGeneProduct: 1167\n",
      "      OrganismTaxon: 386\n",
      "      SequenceVariant: 284\n",
      "\n",
      "  Potential for RE pairs:\n",
      "    Disease: 0\n",
      "    Gene: 1167\n",
      "    Variant: 284\n",
      "    Chemical: 485\n",
      "    Disease-Gene pairs possible: 0\n",
      "    Disease-Variant pairs possible: 0\n",
      "    Gene-Variant pairs possible: 331428\n",
      "\n",
      "  Sample entities from first doc:\n",
      "    - GeneOrGeneProduct: 'Hepatocyte nuclear factor-6' (pos: 0-27, id: 3175)\n",
      "    - DiseaseOrPhenotypicFeature: 'type II diabetes' (pos: 74-90, id: D003924)\n",
      "    - GeneOrGeneProduct: 'insulin' (pos: 140-147, id: 3630)\n",
      "    - GeneOrGeneProduct: 'hepatocyte nuclear factor (HNF)-6' (pos: 184-217, id: 3175)\n",
      "    - DiseaseOrPhenotypicFeature: 'maturity-onset diabetes' (pos: 292-315, id: D003924)\n",
      "\n",
      "  Sample relations from first doc:\n",
      "    - Association: 3175 ↔ D003924\n",
      "    - Positive_Correlation: D005947 ↔ 3630\n",
      "    - Association: D005947 ↔ D003924\n",
      "\n",
      "DEV Documents:\n",
      "  Total documents: 50\n",
      "\n",
      "  First document (PMID: 14510914):\n",
      "    Title: Congenital hypothyroidism due to a new deletion in the sodium/iodide symporter p...\n",
      "    Text length: 1920 chars\n",
      "    Entities: 37\n",
      "    Relations: 12\n",
      "\n",
      "  Entity Statistics:\n",
      "    Documents with entities: 50/50\n",
      "    Total entities: 1535\n",
      "    Entity types:\n",
      "      CellLine: 17\n",
      "      ChemicalEntity: 175\n",
      "      DiseaseOrPhenotypicFeature: 464\n",
      "      GeneOrGeneProduct: 549\n",
      "      OrganismTaxon: 184\n",
      "      SequenceVariant: 146\n",
      "\n",
      "  Potential for RE pairs:\n",
      "    Disease: 0\n",
      "    Gene: 549\n",
      "    Variant: 146\n",
      "    Chemical: 175\n",
      "    Disease-Gene pairs possible: 0\n",
      "    Disease-Variant pairs possible: 0\n",
      "    Gene-Variant pairs possible: 80154\n",
      "\n",
      "  Sample entities from first doc:\n",
      "    - DiseaseOrPhenotypicFeature: 'Congenital hypothyroidism' (pos: 0-25, id: D003409)\n",
      "    - GeneOrGeneProduct: 'sodium/iodide symporter' (pos: 55-78, id: 6528)\n",
      "    - DiseaseOrPhenotypicFeature: 'Iodide transport defect' (pos: 99-122, id: C564766)\n",
      "    - DiseaseOrPhenotypicFeature: 'ITD' (pos: 124-127, id: C564766)\n",
      "    - DiseaseOrPhenotypicFeature: 'inability of the thyroid' (pos: 168-192, id: D050033)\n",
      "\n",
      "  Sample relations from first doc:\n",
      "    - Positive_Correlation: c|DEL|1314_1328| ↔ D003409\n",
      "    - Association: D050033 ↔ D007454\n",
      "    - Positive_Correlation: p|DEL|439_443| ↔ C564766\n",
      "\n",
      "TEST Documents:\n",
      "  Total documents: 50\n",
      "\n",
      "  First document (PMID: 15485686):\n",
      "    Title: A novel SCN5A mutation manifests as a malignant form of long QT syndrome with pe...\n",
      "    Text length: 1815 chars\n",
      "    Entities: 30\n",
      "    Relations: 18\n",
      "\n",
      "  Entity Statistics:\n",
      "    Documents with entities: 50/50\n",
      "    Total entities: 1850\n",
      "    Entity types:\n",
      "      CellLine: 26\n",
      "      ChemicalEntity: 283\n",
      "      DiseaseOrPhenotypicFeature: 479\n",
      "      GeneOrGeneProduct: 684\n",
      "      OrganismTaxon: 198\n",
      "      SequenceVariant: 180\n",
      "\n",
      "  Potential for RE pairs:\n",
      "    Disease: 0\n",
      "    Gene: 684\n",
      "    Variant: 180\n",
      "    Chemical: 283\n",
      "    Disease-Gene pairs possible: 0\n",
      "    Disease-Variant pairs possible: 0\n",
      "    Gene-Variant pairs possible: 123120\n",
      "\n",
      "  Sample entities from first doc:\n",
      "    - GeneOrGeneProduct: 'SCN5A' (pos: 8-13, id: 6331)\n",
      "    - DiseaseOrPhenotypicFeature: 'long QT syndrome' (pos: 56-72, id: D008133)\n",
      "    - DiseaseOrPhenotypicFeature: 'tachycardia' (pos: 97-108, id: D013610)\n",
      "    - DiseaseOrPhenotypicFeature: 'bradycardia' (pos: 109-120, id: D001919)\n",
      "    - DiseaseOrPhenotypicFeature: 'long QT syndrome' (pos: 144-160, id: D008133)\n",
      "\n",
      "  Sample relations from first doc:\n",
      "    - Association: D001919 ↔ 6331\n",
      "    - Positive_Correlation: D001919 ↔ p|SUB|V|1763|M\n",
      "    - Association: D013610 ↔ 6331\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# DIAGNOSTIC: Check Documents Before Conversion\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIAGNOSTIC: Analyzing Loaded Documents\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def diagnose_documents(docs, name):\n",
    "    \"\"\"Analyze documents to see what we have.\"\"\"\n",
    "    print(f\"\\n{name} Documents:\")\n",
    "    print(f\"  Total documents: {len(docs)}\")\n",
    "    \n",
    "    if len(docs) == 0:\n",
    "        print(\"  ❌ No documents loaded!\")\n",
    "        return\n",
    "    \n",
    "    # Check first document\n",
    "    first_doc = docs[0]\n",
    "    print(f\"\\n  First document (PMID: {first_doc['pmid']}):\")\n",
    "    print(f\"    Title: {first_doc['title'][:80]}...\")\n",
    "    print(f\"    Text length: {len(first_doc['text'])} chars\")\n",
    "    print(f\"    Entities: {len(first_doc['entities'])}\")\n",
    "    print(f\"    Relations: {len(first_doc['relations'])}\")\n",
    "    \n",
    "    # Count entity types\n",
    "    entity_types = {}\n",
    "    total_entities = 0\n",
    "    docs_with_entities = 0\n",
    "    \n",
    "    for doc in docs:\n",
    "        if len(doc['entities']) > 0:\n",
    "            docs_with_entities += 1\n",
    "        for ent in doc['entities']:\n",
    "            ent_type = ent['type']\n",
    "            entity_types[ent_type] = entity_types.get(ent_type, 0) + 1\n",
    "            total_entities += 1\n",
    "    \n",
    "    print(f\"\\n  Entity Statistics:\")\n",
    "    print(f\"    Documents with entities: {docs_with_entities}/{len(docs)}\")\n",
    "    print(f\"    Total entities: {total_entities}\")\n",
    "    print(f\"    Entity types:\")\n",
    "    for ent_type, count in sorted(entity_types.items()):\n",
    "        print(f\"      {ent_type}: {count}\")\n",
    "    \n",
    "    # Check for valid pairs\n",
    "    disease = entity_types.get('Disease', 0)\n",
    "    gene = entity_types.get('Gene', 0) + entity_types.get('GeneOrGeneProduct', 0)\n",
    "    variant = entity_types.get('Variant', 0) + entity_types.get('SequenceVariant', 0)\n",
    "    chemical = entity_types.get('Chemical', 0) + entity_types.get('ChemicalEntity', 0)\n",
    "    \n",
    "    print(f\"\\n  Potential for RE pairs:\")\n",
    "    print(f\"    Disease: {disease}\")\n",
    "    print(f\"    Gene: {gene}\")\n",
    "    print(f\"    Variant: {variant}\")\n",
    "    print(f\"    Chemical: {chemical}\")\n",
    "    print(f\"    Disease-Gene pairs possible: {disease * gene}\")\n",
    "    print(f\"    Disease-Variant pairs possible: {disease * variant}\")\n",
    "    print(f\"    Gene-Variant pairs possible: {gene * variant}\")\n",
    "    \n",
    "    # Show sample entities from first doc\n",
    "    if len(first_doc['entities']) > 0:\n",
    "        print(f\"\\n  Sample entities from first doc:\")\n",
    "        for ent in first_doc['entities'][:5]:\n",
    "            print(f\"    - {ent['type']}: '{ent['text']}' (pos: {ent['start']}-{ent['end']}, id: {ent['id']})\")\n",
    "    \n",
    "    # Show sample relations\n",
    "    if len(first_doc['relations']) > 0:\n",
    "        print(f\"\\n  Sample relations from first doc:\")\n",
    "        for rel in first_doc['relations'][:3]:\n",
    "            print(f\"    - {rel['type']}: {rel['arg1']} ↔ {rel['arg2']}\")\n",
    "    \n",
    "    return entity_types\n",
    "\n",
    "# Diagnose all datasets\n",
    "train_entity_types = diagnose_documents(train_docs, \"TRAIN\")\n",
    "dev_entity_types = diagnose_documents(dev_docs, \"DEV\")\n",
    "test_entity_types = diagnose_documents(test_docs, \"TEST\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing BioBERT tokenizer...\n",
      "✓ Tokenizer loaded with 28996 tokens\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# INITIALIZE TOKENIZER\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nInitializing BioBERT tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"✓ Tokenizer loaded with {len(tokenizer)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting data to BERT-GT format...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Converting to BERT-GT format:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  DEBUG: First document\n",
      "    Text: 'Hepatocyte nuclear factor-6: associations between genetic variability and type II diabetes and betwe...'\n",
      "    Entities: 32\n",
      "    Relations: 3\n",
      "    Mapped entities: 32/32\n",
      "    First mapped entity: {'id': '3175', 'type': 'GeneOrGeneProduct', 'start': 1, 'end': 9, 'text': 'Hepatocyte nuclear factor-6'}\n",
      "    Pairs created: 266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to BERT-GT format: 100%|██████████| 100/100 [00:00<00:00, 322.54it/s]\n",
      "Converting to BERT-GT format: 100%|██████████| 50/50 [00:00<00:00, 355.41it/s]\n",
      "Converting to BERT-GT format: 100%|██████████| 50/50 [00:00<00:00, 169.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Converted:\n",
      "  Train: 21682 examples\n",
      "  Dev: 11053 examples\n",
      "  Test: 11936 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CONVERT TO BERT-GT FORMAT\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nConverting data to BERT-GT format...\\n\")\n",
    "\n",
    "converter = BioREDToBERTGTConverter(tokenizer, max_seq_length=MAX_LENGTH)\n",
    "\n",
    "train_examples = converter.convert_documents(train_docs)\n",
    "dev_examples = converter.convert_documents(dev_docs)\n",
    "test_examples = converter.convert_documents(test_docs)\n",
    "\n",
    "print(f\"\\n✓ Converted:\")\n",
    "print(f\"  Train: {len(train_examples)} examples\")\n",
    "print(f\"  Dev: {len(dev_examples)} examples\")\n",
    "print(f\"  Test: {len(test_examples)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating datasets...\n",
      "\n",
      "Dataset sizes:\n",
      "  Train: 21682 examples\n",
      "  Dev: 11053 examples\n",
      "  Test: 11936 examples\n",
      "  Relation types: ['Positive_Correlation', 'Negative_Correlation', 'Association', 'No_Relation']\n",
      "\n",
      "Creating DataLoaders...\n",
      "\n",
      "✓ DataLoaders created:\n",
      "  Train: 5421 batches\n",
      "  Dev: 2764 batches\n",
      "  Test: 2984 batches\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CREATE DATASETS AND DATALOADERS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nCreating datasets...\")\n",
    "\n",
    "train_dataset = BERTGTDataset(train_examples, RELATION_TYPES)\n",
    "dev_dataset = BERTGTDataset(dev_examples, RELATION_TYPES)\n",
    "test_dataset = BERTGTDataset(test_examples, RELATION_TYPES)\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Train: {len(train_dataset)} examples\")\n",
    "print(f\"  Dev: {len(dev_dataset)} examples\")\n",
    "print(f\"  Test: {len(test_dataset)} examples\")\n",
    "print(f\"  Relation types: {train_dataset.relation_types}\")\n",
    "\n",
    "# Create DataLoaders\n",
    "print(\"\\nCreating DataLoaders...\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=bert_gt_collate_fn\n",
    ")\n",
    "\n",
    "dev_loader = DataLoader(\n",
    "    dev_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=bert_gt_collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=bert_gt_collate_fn\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ DataLoaders created:\")\n",
    "print(f\"  Train: {len(train_loader)} batches\")\n",
    "print(f\"  Dev: {len(dev_loader)} batches\")\n",
    "print(f\"  Test: {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Initialize and Train BERT-GT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing BERT-GT model...\n",
      "\n",
      "✓ BERT-GT Model initialized:\n",
      "  Total parameters: 125,440,516\n",
      "  Trainable parameters: 125,440,516\n",
      "  Hidden size: 768\n",
      "  Graph layers: 2\n",
      "  Attention heads: 4\n",
      "  Max entities: 20\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# INITIALIZE BERT-GT MODEL\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nInitializing BERT-GT model...\")\n",
    "\n",
    "model = BERTGTModel(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_relations=len(RELATION_TYPES),\n",
    "    num_graph_layers=NUM_GRAPH_LAYERS,\n",
    "    num_attention_heads=NUM_ATTENTION_HEADS,\n",
    "    max_entities=MAX_ENTITIES,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n✓ BERT-GT Model initialized:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Hidden size: {model.hidden_size}\")\n",
    "print(f\"  Graph layers: {NUM_GRAPH_LAYERS}\")\n",
    "print(f\"  Attention heads: {NUM_ATTENTION_HEADS}\")\n",
    "print(f\"  Max entities: {MAX_ENTITIES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING BERT-GT MODEL\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Epoch 1/3\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 252/5421 [1:02:25<21:20:26, 14.86s/it, loss=1.16]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTRAINING BERT-GT MODEL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m training_stats \u001b[38;5;241m=\u001b[39m train_bert_gt(\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     11\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[1;32m     12\u001b[0m     val_loader\u001b[38;5;241m=\u001b[39mdev_loader,\n\u001b[1;32m     13\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39mNUM_EPOCHS,\n\u001b[1;32m     14\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39mLEARNING_RATE,\n\u001b[1;32m     15\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✓ Training complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 39\u001b[0m, in \u001b[0;36mtrain_bert_gt\u001b[0;34m(model, train_loader, val_loader, num_epochs, learning_rate, device)\u001b[0m\n\u001b[1;32m     36\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m     40\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m     41\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m     42\u001b[0m     entity1_start\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity1_start\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     43\u001b[0m     entity1_end\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity1_end\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     44\u001b[0m     entity2_start\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity2_start\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     45\u001b[0m     entity2_end\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity2_end\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     46\u001b[0m     entities\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentities\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     47\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     50\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     51\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 114\u001b[0m, in \u001b[0;36mBERTGTModel.forward\u001b[0;34m(self, input_ids, attention_mask, entity1_start, entity1_end, entity2_start, entity2_end, entities, labels)\u001b[0m\n\u001b[1;32m    111\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Get BERT representations\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(\n\u001b[1;32m    115\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    116\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask\n\u001b[1;32m    117\u001b[0m )\n\u001b[1;32m    119\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state  \u001b[38;5;66;03m# [batch_size, seq_length, hidden_size]\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Extract entity representations\u001b[39;00m\n",
      "File \u001b[0;32m/share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1020\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1011\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1013\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1014\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1015\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1019\u001b[0m )\n\u001b[0;32m-> 1020\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1021\u001b[0m     embedding_output,\n\u001b[1;32m   1022\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   1023\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1024\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1025\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[1;32m   1026\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1027\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1028\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1029\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1030\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1031\u001b[0m )\n\u001b[1;32m   1032\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1033\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    601\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    603\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    608\u001b[0m     )\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 610\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[1;32m    611\u001b[0m         hidden_states,\n\u001b[1;32m    612\u001b[0m         attention_mask,\n\u001b[1;32m    613\u001b[0m         layer_head_mask,\n\u001b[1;32m    614\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    615\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    616\u001b[0m         past_key_value,\n\u001b[1;32m    617\u001b[0m         output_attentions,\n\u001b[1;32m    618\u001b[0m     )\n\u001b[1;32m    620\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:537\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    534\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    535\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 537\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward_chunk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size_feed_forward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len_dim, attention_output\n\u001b[1;32m    539\u001b[0m )\n\u001b[1;32m    540\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    542\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m/share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages/transformers/pytorch_utils.py:236\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m forward_fn(\u001b[38;5;241m*\u001b[39minput_tensors)\n",
      "File \u001b[0;32m/share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:550\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    549\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 550\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:462\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 462\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[1;32m    463\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    464\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m/share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/share/apps/rc/software/Anaconda3/2023.07-2/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TRAIN BERT-GT MODEL\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING BERT-GT MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "training_stats = train_bert_gt(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=dev_loader,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LOAD BEST MODEL AND EVALUATE\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nLoading best model...\")\n",
    "model.load_state_dict(torch.load('best_bert_gt_model.pt'))\n",
    "print(\"✓ Best model loaded\")\n",
    "\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_metrics = evaluate_bert_gt(model, test_loader, device)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Test F1 Score: {test_metrics['f1']:.4f}\")\n",
    "print(f\"Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Convert to DataFrame\n",
    "stats_df = pd.DataFrame(training_stats)\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training loss\n",
    "ax1.plot(stats_df['epoch'], stats_df['train_loss'], marker='o', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Training Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation metrics\n",
    "ax2.plot(stats_df['epoch'], stats_df['val_f1'], marker='o', linewidth=2, label='F1 Score')\n",
    "ax2.plot(stats_df['epoch'], stats_df['val_accuracy'], marker='s', linewidth=2, label='Accuracy')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Score', fontsize=12)\n",
    "ax2.set_title('Validation Metrics Over Time', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('bert_gt_training_progress.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Training progress visualized\")\n",
    "print(\"  Saved as: bert_gt_training_progress.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Training Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save statistics\n",
    "stats_df.to_csv('bert_gt_training_stats.csv', index=False)\n",
    "print(\"\\n✓ Training statistics saved to: bert_gt_training_stats.csv\")\n",
    "\n",
    "# Display final stats\n",
    "print(\"\\nTraining Statistics:\")\n",
    "print(stats_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Summary\n",
    "\n",
    "Summary of BERT-GT model performance and comparison with baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BERT-GT MODEL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(f\"  Base Model: BioBERT v1.1\")\n",
    "print(f\"  Graph Transformer Layers: {NUM_GRAPH_LAYERS}\")\n",
    "print(f\"  Attention Heads: {NUM_ATTENTION_HEADS}\")\n",
    "print(f\"  Max Entities per Document: {MAX_ENTITIES}\")\n",
    "print(f\"  Total Parameters: {total_params:,}\")\n",
    "\n",
    "print(\"\\nTraining Configuration:\")\n",
    "print(f\"  Training Documents: {len(train_docs)}\")\n",
    "print(f\"  Training Examples: {len(train_dataset)}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(f\"  F1 Score: {test_metrics['f1']:.4f}\")\n",
    "print(f\"  Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nExpected Performance (Full Dataset):\")\n",
    "print(\"  Entity Pair F1: ~73%\")\n",
    "print(\"  + Relation Type F1: ~59%\")\n",
    "\n",
    "print(\"\\nModel Files:\")\n",
    "print(\"  Best Model: best_bert_gt_model.pt\")\n",
    "print(\"  Training Stats: bert_gt_training_stats.csv\")\n",
    "print(\"  Training Plot: bert_gt_training_progress.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
