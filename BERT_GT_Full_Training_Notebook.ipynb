{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT-GT for BioRED Relation Extraction\n",
    "\n",
    "This notebook implements **BERT-GT (BERT with Graph Transformer)** - the official model from the BioRED paper.\n",
    "\n",
    "## What is BERT-GT?\n",
    "\n",
    "BERT-GT improves relation extraction by:\n",
    "- **Graph Transformer layers** that model entity interactions\n",
    "- **Document-level reasoning** for cross-sentence relations\n",
    "- **Entity-aware attention** focusing on entity pairs\n",
    "\n",
    "## Performance\n",
    "\n",
    "| Model | Entity Pair F1 | + Type F1 |\n",
    "|-------|----------------|------------|\n",
    "| BioBERT | ~65% | ~52% |\n",
    "| **BERT-GT** | **~73%** | **~59%** |\n",
    "\n",
    "**Reference**: https://github.com/ncbi/bert_gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install transformers torch scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version in PyTorch: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "print(f\"GPU index in use: {torch.cuda.current_device()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Graph Transformer Layer\n",
    "\n",
    "The core innovation of BERT-GT: treats entities as nodes in a graph and uses graph attention to model their relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphTransformerLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Transformer Layer for modeling entity interactions.\n",
    "    \n",
    "    This layer treats entities as nodes in a graph and uses\n",
    "    graph attention to model their relationships.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, num_heads=4, dropout=0.1):\n",
    "        super(GraphTransformerLayer, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        \n",
    "        assert hidden_size % num_heads == 0, \"hidden_size must be divisible by num_heads\"\n",
    "        \n",
    "        # Multi-head attention for graph\n",
    "        self.query = nn.Linear(hidden_size, hidden_size)\n",
    "        self.key = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size * 4, hidden_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, entity_reprs, adjacency_matrix=None):\n",
    "        \"\"\"\n",
    "        Apply graph transformer layer.\n",
    "        \n",
    "        Args:\n",
    "            entity_reprs: [batch_size, num_entities, hidden_size]\n",
    "            adjacency_matrix: [batch_size, num_entities, num_entities] (optional)\n",
    "        \n",
    "        Returns:\n",
    "            Updated entity representations [batch_size, num_entities, hidden_size]\n",
    "        \"\"\"\n",
    "        batch_size, num_entities, hidden_size = entity_reprs.size()\n",
    "        \n",
    "        # Multi-head self-attention on graph\n",
    "        Q = self.query(entity_reprs).view(batch_size, num_entities, self.num_heads, self.head_dim)\n",
    "        K = self.key(entity_reprs).view(batch_size, num_entities, self.num_heads, self.head_dim)\n",
    "        V = self.value(entity_reprs).view(batch_size, num_entities, self.num_heads, self.head_dim)\n",
    "        \n",
    "        Q = Q.transpose(1, 2)  # [batch_size, num_heads, num_entities, head_dim]\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Apply adjacency matrix if provided (mask non-neighbors)\n",
    "        if adjacency_matrix is not None:\n",
    "            adjacency_matrix = adjacency_matrix.unsqueeze(1)  # [batch_size, 1, num_entities, num_entities]\n",
    "            attention_scores = attention_scores.masked_fill(adjacency_matrix == 0, -1e9)\n",
    "        \n",
    "        # Attention weights\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        context = torch.matmul(attention_probs, V)  # [batch_size, num_heads, num_entities, head_dim]\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, num_entities, hidden_size)\n",
    "        \n",
    "        # Output projection + residual\n",
    "        output = self.out_proj(context)\n",
    "        output = self.dropout(output)\n",
    "        entity_reprs = self.layer_norm1(entity_reprs + output)\n",
    "        \n",
    "        # Feed-forward + residual\n",
    "        ffn_output = self.ffn(entity_reprs)\n",
    "        entity_reprs = self.layer_norm2(entity_reprs + ffn_output)\n",
    "        \n",
    "        return entity_reprs\n",
    "\n",
    "print(\"✓ GraphTransformerLayer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BERT-GT Model\n",
    "\n",
    "Complete BERT-GT architecture:\n",
    "1. BERT encoder for token representations\n",
    "2. Entity extraction and projection\n",
    "3. Graph Transformer layers\n",
    "4. Entity pair classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTGTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete BERT-GT model for document-level relation extraction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name='dmis-lab/biobert-v1.1',\n",
    "        num_relations=4,\n",
    "        num_graph_layers=2,\n",
    "        num_attention_heads=4,\n",
    "        max_entities=20,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super(BERTGTModel, self).__init__()\n",
    "        \n",
    "        # BioBERT encoder\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        self.max_entities = max_entities\n",
    "        \n",
    "        # Entity representation layer\n",
    "        self.entity_projection = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "        # Graph Transformer layers\n",
    "        self.graph_layers = nn.ModuleList([\n",
    "            GraphTransformerLayer(\n",
    "                self.hidden_size,\n",
    "                num_heads=num_attention_heads,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(num_graph_layers)\n",
    "        ])\n",
    "        \n",
    "        # Entity pair combination: [e1, e2, e1*e2]\n",
    "        self.pair_projection = nn.Linear(self.hidden_size * 3, self.hidden_size)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Relation classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.hidden_size, num_relations)\n",
    "        )\n",
    "    \n",
    "    def extract_entity_representations(self, sequence_output, entity_positions, batch_size):\n",
    "        \"\"\"\n",
    "        Extract entity representations from BERT output using average pooling.\n",
    "        \"\"\"\n",
    "        all_entity_reprs = []\n",
    "        entity_masks = []\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            batch_entities = entity_positions[b]\n",
    "            entity_reprs = []\n",
    "            \n",
    "            for entity in batch_entities[:self.max_entities]:\n",
    "                start = entity['start']\n",
    "                end = entity['end'] + 1  # Inclusive end\n",
    "                \n",
    "                # Average pooling over entity tokens\n",
    "                entity_tokens = sequence_output[b, start:end, :]\n",
    "                if entity_tokens.size(0) > 0:\n",
    "                    entity_repr = entity_tokens.mean(dim=0)\n",
    "                else:\n",
    "                    entity_repr = torch.zeros(self.hidden_size, device=sequence_output.device)\n",
    "                \n",
    "                entity_reprs.append(entity_repr)\n",
    "            \n",
    "            # Pad to max_entities\n",
    "            num_entities = len(entity_reprs)\n",
    "            mask = [1] * num_entities + [0] * (self.max_entities - num_entities)\n",
    "            \n",
    "            while len(entity_reprs) < self.max_entities:\n",
    "                entity_reprs.append(torch.zeros(self.hidden_size, device=sequence_output.device))\n",
    "            \n",
    "            all_entity_reprs.append(torch.stack(entity_reprs[:self.max_entities]))\n",
    "            entity_masks.append(mask)\n",
    "        \n",
    "        entity_reprs = torch.stack(all_entity_reprs)  # [batch_size, max_entities, hidden_size]\n",
    "        entity_masks = torch.tensor(entity_masks, device=sequence_output.device)\n",
    "        \n",
    "        return entity_reprs, entity_masks\n",
    "    \n",
    "    def build_adjacency_matrix(self, entity_positions, batch_size):\n",
    "        \"\"\"\n",
    "        Build adjacency matrix for entity graph.\n",
    "        Uses fully connected graph (all entities can attend to each other).\n",
    "        \"\"\"\n",
    "        adjacency_matrices = []\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            num_entities = min(len(entity_positions[b]), self.max_entities)\n",
    "            \n",
    "            # Fully connected graph\n",
    "            adj_matrix = torch.zeros(self.max_entities, self.max_entities)\n",
    "            adj_matrix[:num_entities, :num_entities] = 1\n",
    "            \n",
    "            adjacency_matrices.append(adj_matrix)\n",
    "        \n",
    "        return torch.stack(adjacency_matrices).to(self.bert.device)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, entity1_start, entity1_end, \n",
    "                entity2_start, entity2_end, entities, labels=None):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \"\"\"\n",
    "        batch_size = input_ids.size(0)\n",
    "        \n",
    "        # Get BERT representations\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        sequence_output = outputs.last_hidden_state  # [batch_size, seq_length, hidden_size]\n",
    "        \n",
    "        # Extract entity representations\n",
    "        entity_reprs, entity_mask = self.extract_entity_representations(\n",
    "            sequence_output, entities, batch_size\n",
    "        )\n",
    "        \n",
    "        # Project entity representations\n",
    "        entity_reprs = self.entity_projection(entity_reprs)\n",
    "        entity_reprs = self.dropout(entity_reprs)\n",
    "        \n",
    "        # Build adjacency matrix\n",
    "        adjacency_matrix = self.build_adjacency_matrix(entities, batch_size)\n",
    "        \n",
    "        # Apply Graph Transformer layers\n",
    "        for graph_layer in self.graph_layers:\n",
    "            entity_reprs = graph_layer(entity_reprs, adjacency_matrix)\n",
    "        \n",
    "        # Get target entity pair representations\n",
    "        pair_reprs = []\n",
    "        for b in range(batch_size):\n",
    "            # Find target entities\n",
    "            target_ent1_start = entity1_start[b]\n",
    "            target_ent2_start = entity2_start[b]\n",
    "            \n",
    "            ent1_idx = None\n",
    "            ent2_idx = None\n",
    "            for i, entity in enumerate(entities[b][:self.max_entities]):\n",
    "                if entity['start'] == target_ent1_start:\n",
    "                    ent1_idx = i\n",
    "                if entity['start'] == target_ent2_start:\n",
    "                    ent2_idx = i\n",
    "            \n",
    "            # Get representations\n",
    "            if ent1_idx is not None and ent2_idx is not None:\n",
    "                e1_repr = entity_reprs[b, ent1_idx]\n",
    "                e2_repr = entity_reprs[b, ent2_idx]\n",
    "            else:\n",
    "                e1_repr = torch.zeros(self.hidden_size, device=entity_reprs.device)\n",
    "                e2_repr = torch.zeros(self.hidden_size, device=entity_reprs.device)\n",
    "            \n",
    "            # Combine: [e1, e2, e1*e2]\n",
    "            pair_repr = torch.cat([e1_repr, e2_repr, e1_repr * e2_repr], dim=0)\n",
    "            pair_reprs.append(pair_repr)\n",
    "        \n",
    "        pair_reprs = torch.stack(pair_reprs)  # [batch_size, hidden_size * 3]\n",
    "        \n",
    "        # Project and classify\n",
    "        pair_repr = self.pair_projection(pair_reprs)\n",
    "        pair_repr = self.dropout(pair_repr)\n",
    "        logits = self.classifier(pair_repr)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "        \n",
    "        return {'loss': loss, 'logits': logits} if loss is not None else {'logits': logits}\n",
    "\n",
    "print(\"✓ BERTGTModel defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Converter\n",
    "\n",
    "Convert BioRED PubTator format to BERT-GT format with entity positions and graph structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioREDToBERTGTConverter:\n",
    "    \"\"\"\n",
    "    Converts BioRED documents to BERT-GT format.\n",
    "    FIXED VERSION - handles various entity type names.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, max_seq_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self._debug_mode = True  # Set to False after testing\n",
    "    \n",
    "    def convert_documents(self, documents):\n",
    "        \"\"\"\n",
    "        Convert BioRED documents to BERT-GT format.\n",
    "        \"\"\"\n",
    "        bert_gt_examples = []\n",
    "        \n",
    "        for doc_idx, doc in enumerate(tqdm(documents, desc=\"Converting to BERT-GT format\")):\n",
    "            # Get text and entities\n",
    "            text = doc['text']\n",
    "            entities = doc['entities']\n",
    "            relations = doc['relations']\n",
    "            \n",
    "            if self._debug_mode and doc_idx == 0:\n",
    "                print(f\"\\n  DEBUG: First document\")\n",
    "                print(f\"    Text: '{text[:100]}...'\")\n",
    "                print(f\"    Entities: {len(entities)}\")\n",
    "                print(f\"    Relations: {len(relations)}\")\n",
    "            \n",
    "            # Tokenize\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                max_length=self.max_seq_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_offsets_mapping=True\n",
    "            )\n",
    "            \n",
    "            # Map entities to token positions\n",
    "            token_to_char = encoding['offset_mapping']\n",
    "            entity_token_positions = []\n",
    "            \n",
    "            for ent in entities:\n",
    "                char_start = ent['start']\n",
    "                char_end = ent['end']\n",
    "                \n",
    "                # Find token positions\n",
    "                token_start = None\n",
    "                token_end = None\n",
    "                \n",
    "                for i, (t_start, t_end) in enumerate(token_to_char):\n",
    "                    if t_start <= char_start < t_end and token_start is None:\n",
    "                        token_start = i\n",
    "                    if t_start < char_end <= t_end:\n",
    "                        token_end = i\n",
    "                        break\n",
    "                \n",
    "                # Also accept if token overlaps with entity\n",
    "                if token_start is None or token_end is None:\n",
    "                    for i, (t_start, t_end) in enumerate(token_to_char):\n",
    "                        # Check overlap\n",
    "                        if t_start <= char_start < t_end:\n",
    "                            token_start = i\n",
    "                        if t_start <= char_end <= t_end or (t_start < char_end and char_end <= t_end):\n",
    "                            token_end = i\n",
    "                            break\n",
    "                \n",
    "                if token_start is not None and token_end is not None:\n",
    "                    entity_token_positions.append({\n",
    "                        'id': ent['id'],\n",
    "                        'type': ent['type'],\n",
    "                        'start': token_start,\n",
    "                        'end': token_end,\n",
    "                        'text': ent['text']\n",
    "                    })\n",
    "            \n",
    "            if self._debug_mode and doc_idx == 0:\n",
    "                print(f\"    Mapped entities: {len(entity_token_positions)}/{len(entities)}\")\n",
    "                if len(entity_token_positions) > 0:\n",
    "                    print(f\"    First mapped entity: {entity_token_positions[0]}\")\n",
    "            \n",
    "            # Create entity pairs\n",
    "            pairs_created = 0\n",
    "            for i, ent1 in enumerate(entity_token_positions):\n",
    "                for ent2 in entity_token_positions[i+1:]:\n",
    "                    # Check if valid pair\n",
    "                    if not self._is_valid_pair(ent1['type'], ent2['type']):\n",
    "                        continue\n",
    "                    \n",
    "                    # Find relation label\n",
    "                    relation_label = self._find_relation(ent1['id'], ent2['id'], relations)\n",
    "                    \n",
    "                    # Create example\n",
    "                    example = {\n",
    "                        'input_ids': encoding['input_ids'],\n",
    "                        'attention_mask': encoding['attention_mask'],\n",
    "                        'entity1_start': ent1['start'],\n",
    "                        'entity1_end': ent1['end'],\n",
    "                        'entity1_type': ent1['type'],\n",
    "                        'entity2_start': ent2['start'],\n",
    "                        'entity2_end': ent2['end'],\n",
    "                        'entity2_type': ent2['type'],\n",
    "                        'entities': entity_token_positions,\n",
    "                        'label': relation_label,\n",
    "                        'pmid': doc['pmid']\n",
    "                    }\n",
    "                    \n",
    "                    bert_gt_examples.append(example)\n",
    "                    pairs_created += 1\n",
    "            \n",
    "            if self._debug_mode and doc_idx == 0:\n",
    "                print(f\"    Pairs created: {pairs_created}\")\n",
    "        \n",
    "        # Turn off debug after first document\n",
    "        self._debug_mode = False\n",
    "        \n",
    "        return bert_gt_examples\n",
    "    \n",
    "    def _is_valid_pair(self, type1, type2):\n",
    "        \"\"\"Check if entity pair is valid for RE.\"\"\"\n",
    "        \n",
    "        # Normalize entity types\n",
    "        def normalize_type(t):\n",
    "            t = t.lower()\n",
    "            # Map various type names to standard names\n",
    "            if 'gene' in t or 'protein' in t:\n",
    "                return 'gene'\n",
    "            if 'disease' in t or 'phenotype' in t:\n",
    "                return 'disease'\n",
    "            if 'variant' in t or 'mutation' in t or 'snp' in t:\n",
    "                return 'variant'\n",
    "            if 'chemical' in t or 'drug' in t:\n",
    "                return 'chemical'\n",
    "            return t\n",
    "        \n",
    "        t1 = normalize_type(type1)\n",
    "        t2 = normalize_type(type2)\n",
    "        \n",
    "        # Valid pairs\n",
    "        valid_pairs = [\n",
    "            ('disease', 'gene'),\n",
    "            ('gene', 'disease'),\n",
    "            ('disease', 'variant'),\n",
    "            ('variant', 'disease'),\n",
    "            ('gene', 'variant'),\n",
    "            ('variant', 'gene'),\n",
    "            ('chemical', 'disease'),\n",
    "            ('disease', 'chemical'),\n",
    "            ('chemical', 'gene'),\n",
    "            ('gene', 'chemical'),\n",
    "        ]\n",
    "        \n",
    "        return (t1, t2) in valid_pairs\n",
    "    \n",
    "    def _find_relation(self, id1, id2, relations):\n",
    "        \"\"\"Find relation between entities.\"\"\"\n",
    "        for rel in relations:\n",
    "            # Handle different relation formats\n",
    "            arg1_id = rel['arg1'].split(':')[-1] if ':' in rel['arg1'] else rel['arg1']\n",
    "            arg2_id = rel['arg2'].split(':')[-1] if ':' in rel['arg2'] else rel['arg2']\n",
    "            \n",
    "            if (arg1_id == id1 and arg2_id == id2) or (arg1_id == id2 and arg2_id == id1):\n",
    "                return rel['type']\n",
    "        \n",
    "        return 'No_Relation'\n",
    "\n",
    "print(\"✓ Fixed BioREDToBERTGTConverter defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset and DataLoader\n",
    "\n",
    "PyTorch Dataset for BERT-GT with custom collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTGTDataset(Dataset):\n",
    "    \"\"\"Dataset for BERT-GT model.\"\"\"\n",
    "    \n",
    "    def __init__(self, examples, relation2id):\n",
    "        self.examples = examples\n",
    "        self.relation2id = relation2id\n",
    "        self.id2relation = {v: k for k, v in relation2id.items()}\n",
    "        self.num_relations = len(relation2id)\n",
    "        self.relation_types = list(relation2id.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        \n",
    "        # Convert relation to ID\n",
    "        label = self.relation2id.get(example['label'], self.relation2id['No_Relation'])\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(example['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(example['attention_mask'], dtype=torch.long),\n",
    "            'entity1_start': example['entity1_start'],\n",
    "            'entity1_end': example['entity1_end'],\n",
    "            'entity2_start': example['entity2_start'],\n",
    "            'entity2_end': example['entity2_end'],\n",
    "            'entities': example['entities'],\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def bert_gt_collate_fn(batch):\n",
    "    \"\"\"Custom collate function for BERT-GT.\"\"\"\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.stack([item['label'] for item in batch])\n",
    "    \n",
    "    entity1_start = [item['entity1_start'] for item in batch]\n",
    "    entity1_end = [item['entity1_end'] for item in batch]\n",
    "    entity2_start = [item['entity2_start'] for item in batch]\n",
    "    entity2_end = [item['entity2_end'] for item in batch]\n",
    "    entities = [item['entities'] for item in batch]\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'entity1_start': entity1_start,\n",
    "        'entity1_end': entity1_end,\n",
    "        'entity2_start': entity2_start,\n",
    "        'entity2_end': entity2_end,\n",
    "        'entities': entities,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "print(\"✓ BERTGTDataset and collate_fn defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert_gt(model, train_loader, val_loader, num_epochs=10, learning_rate=1e-5, device='cuda'):\n",
    "    \"\"\"\n",
    "    Train BERT-GT model.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Scheduler\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_f1 = 0\n",
    "    training_stats = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc='Training')\n",
    "        for batch in train_pbar:\n",
    "            # Move to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                entity1_start=batch['entity1_start'],\n",
    "                entity1_end=batch['entity1_end'],\n",
    "                entity2_start=batch['entity2_start'],\n",
    "                entity2_end=batch['entity2_end'],\n",
    "                entities=batch['entities'],\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs['loss']\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        print(f\"\\nAverage training loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        val_metrics = evaluate_bert_gt(model, val_loader, device)\n",
    "        \n",
    "        print(f\"Validation F1: {val_metrics['f1']:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['f1'] > best_val_f1:\n",
    "            best_val_f1 = val_metrics['f1']\n",
    "            torch.save(model.state_dict(), 'best_bert_gt_model.pt')\n",
    "            print(f\"✓ Saved best model with F1: {best_val_f1:.4f}\")\n",
    "        \n",
    "        training_stats.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_f1': val_metrics['f1'],\n",
    "            'val_accuracy': val_metrics['accuracy']\n",
    "        })\n",
    "    \n",
    "    return training_stats\n",
    "\n",
    "\n",
    "def evaluate_bert_gt(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate BERT-GT model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc='Evaluating'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                entity1_start=batch['entity1_start'],\n",
    "                entity1_end=batch['entity1_end'],\n",
    "                entity2_start=batch['entity2_start'],\n",
    "                entity2_end=batch['entity2_end'],\n",
    "                entities=batch['entities']\n",
    "            )\n",
    "            \n",
    "            logits = outputs['logits']\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return {'f1': f1, 'accuracy': accuracy}\n",
    "\n",
    "print(\"✓ Training and evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load and Prepare Data\n",
    "\n",
    "Now we'll load the BioRED data and convert it to BERT-GT format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "# Set this to True for testing, False for full training\n",
    "TESTING = False\n",
    "\n",
    "# File paths (UPDATE THESE!)\n",
    "TRAIN_DATA_PATH = './data/Train.PubTator'\n",
    "DEV_DATA_PATH = './data/Dev.PubTator'\n",
    "TEST_DATA_PATH = './data/Test.PubTator'\n",
    "\n",
    "# Model settings\n",
    "MODEL_NAME = 'dmis-lab/biobert-v1.1'\n",
    "MAX_LENGTH = 512\n",
    "NUM_GRAPH_LAYERS = 2  # Number of Graph Transformer layers\n",
    "NUM_ATTENTION_HEADS = 4\n",
    "MAX_ENTITIES = 20\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# Training settings\n",
    "if TESTING:\n",
    "    print(\"⚠️  TESTING MODE\")\n",
    "    MAX_DOCS = 100  # Load 100 docs for testing\n",
    "    NUM_EPOCHS = 3\n",
    "    BATCH_SIZE = 4\n",
    "    LEARNING_RATE = 1e-5\n",
    "else:\n",
    "    print(\"✓ FULL TRAINING MODE\")\n",
    "    MAX_DOCS = None  # Load all documents\n",
    "    NUM_EPOCHS = 30\n",
    "    BATCH_SIZE = 8\n",
    "    LEARNING_RATE = 1e-5\n",
    "\n",
    "# Relation types\n",
    "RELATION_TYPES = {\n",
    "    'Positive_Correlation': 0,\n",
    "    'Negative_Correlation': 1,\n",
    "    'Association': 2,\n",
    "    'No_Relation': 3\n",
    "}\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Max documents: {MAX_DOCS or 'ALL'}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Graph layers: {NUM_GRAPH_LAYERS}\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "#              LOAD BIORED DATA \n",
    "# ============================================\n",
    "\n",
    "class BioREDDataLoader:\n",
    "    \"\"\"\n",
    "    Loads and parses BioRED dataset in PubTator format.\n",
    "    \n",
    "    PubTator format:\n",
    "    - First line: PMID|t|Title\n",
    "    - Second line: PMID|a|Abstract\n",
    "    - Following lines: PMID\\tstart\\tend\\ttext\\ttype\\tid\n",
    "    - Relation lines: PMID\\trelation_type\\tArg1:entity_id\\tArg2:entity_id\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to .pubtator file, directory containing .pubtator files, or list of paths\n",
    "        max_documents: Maximum number of documents to load (None = load all)\n",
    "        skip_documents: Number of documents to skip from the beginning\n",
    "        verbose: Print loading progress\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_path, max_documents=None, skip_documents=0, verbose=True):\n",
    "        self.file_path = file_path\n",
    "        self.max_documents = max_documents\n",
    "        self.skip_documents = skip_documents\n",
    "        self.verbose = verbose\n",
    "        self.documents = []\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and parse BioRED data from file(s).\"\"\"\n",
    "        # Determine if file_path is a file, directory, or list\n",
    "        if isinstance(self.file_path, list):\n",
    "            # List of file paths\n",
    "            file_paths = self.file_path\n",
    "            if self.verbose:\n",
    "                print(f\"Loading from {len(file_paths)} file(s)\")\n",
    "        elif os.path.isdir(self.file_path):\n",
    "            # Directory - get all .pubtator files\n",
    "            file_paths = sorted([\n",
    "                os.path.join(self.file_path, f) \n",
    "                for f in os.listdir(self.file_path) \n",
    "                if f.endswith('.pubtator')\n",
    "            ])\n",
    "            if self.verbose:\n",
    "                print(f\"Found {len(file_paths)} .pubtator file(s) in directory\")\n",
    "        else:\n",
    "            # Single file\n",
    "            file_paths = [self.file_path]\n",
    "        \n",
    "        if not file_paths:\n",
    "            print(\"Warning: No .pubtator files found!\")\n",
    "            return self.documents\n",
    "        \n",
    "        # Load documents from all files\n",
    "        docs_processed = 0\n",
    "        \n",
    "        for file_path in file_paths:\n",
    "            if self.verbose:\n",
    "                print(f\"\\nLoading: {os.path.basename(file_path)}\")\n",
    "            \n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read().strip()\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Error: File not found - {file_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Split by empty lines (documents are separated by empty lines)\n",
    "            doc_blocks = content.split('\\n\\n')\n",
    "            \n",
    "            for block in doc_blocks:\n",
    "                # Check if we should skip this document\n",
    "                if docs_processed < self.skip_documents:\n",
    "                    docs_processed += 1\n",
    "                    continue\n",
    "                \n",
    "                # Check if we've reached the maximum\n",
    "                if self.max_documents is not None and len(self.documents) >= self.max_documents:\n",
    "                    if self.verbose:\n",
    "                        print(f\"\\n✓ Reached max_documents limit ({self.max_documents})\")\n",
    "                    return self.documents\n",
    "                \n",
    "                if not block.strip():\n",
    "                    continue\n",
    "                    \n",
    "                doc = self._parse_document(block)\n",
    "                if doc:\n",
    "                    self.documents.append(doc)\n",
    "                    docs_processed += 1\n",
    "                    \n",
    "                    # Print progress every 100 documents\n",
    "                    if self.verbose and len(self.documents) % 100 == 0:\n",
    "                        print(f\"  Loaded {len(self.documents)} documents...\", end='\\r')\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\\n✓ Loaded {len(self.documents)} documents total\")\n",
    "            if self.skip_documents > 0:\n",
    "                print(f\"  (Skipped first {self.skip_documents} documents)\")\n",
    "        \n",
    "        return self.documents\n",
    "    \n",
    "    def _parse_document(self, block):\n",
    "        \"\"\"Parse a single document block.\"\"\"\n",
    "        lines = block.strip().split('\\n')\n",
    "        if len(lines) < 2:\n",
    "            return None\n",
    "        \n",
    "        # Parse title and abstract\n",
    "        title_parts = lines[0].split('|t|')\n",
    "        abstract_parts = lines[1].split('|a|')\n",
    "        \n",
    "        if len(title_parts) < 2 or len(abstract_parts) < 2:\n",
    "            return None\n",
    "        \n",
    "        pmid = title_parts[0]\n",
    "        title = title_parts[1] if len(title_parts) > 1 else \"\"\n",
    "        abstract = abstract_parts[1] if len(abstract_parts) > 1 else \"\"\n",
    "        \n",
    "        text = title + \" \" + abstract\n",
    "        \n",
    "        # Parse entities and relations\n",
    "        entities = []\n",
    "        relations = []\n",
    "        \n",
    "        for line in lines[2:]:\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) >= 6:  # Entity annotation\n",
    "                entity = {\n",
    "                    'pmid': parts[0],\n",
    "                    'start': int(parts[1]),\n",
    "                    'end': int(parts[2]),\n",
    "                    'text': parts[3],\n",
    "                    'type': parts[4],\n",
    "                    'id': parts[5]\n",
    "                }\n",
    "                entities.append(entity)\n",
    "            elif len(parts) >= 4 and 'CID' not in parts[1]:  # Relation annotation\n",
    "                relation = {\n",
    "                    'pmid': parts[0],\n",
    "                    'type': parts[1],\n",
    "                    'arg1': parts[2],\n",
    "                    'arg2': parts[3]\n",
    "                }\n",
    "                relations.append(relation)\n",
    "        \n",
    "        return {\n",
    "            'pmid': pmid,\n",
    "            'title': title,\n",
    "            'abstract': abstract,\n",
    "            'text': text,\n",
    "            'entities': entities,\n",
    "            'relations': relations\n",
    "        }\n",
    "\n",
    "print(\"Loading BioRED data...\\n\")\n",
    "\n",
    "train_loader_data = BioREDDataLoader(TRAIN_DATA_PATH, max_documents=MAX_DOCS)\n",
    "dev_loader_data = BioREDDataLoader(DEV_DATA_PATH, max_documents=MAX_DOCS//2 if MAX_DOCS else None)\n",
    "test_loader_data = BioREDDataLoader(TEST_DATA_PATH, max_documents=MAX_DOCS//2 if MAX_DOCS else None)\n",
    "\n",
    "train_docs = train_loader_data.load_data()\n",
    "dev_docs = dev_loader_data.load_data()\n",
    "test_docs = test_loader_data.load_data()\n",
    "\n",
    "print(f\"\\n✓ Loaded:\")\n",
    "print(f\"  Train: {len(train_docs)} documents\")\n",
    "print(f\"  Dev: {len(dev_docs)} documents\")\n",
    "print(f\"  Test: {len(test_docs)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DIAGNOSTIC: Check Documents Before Conversion\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIAGNOSTIC: Analyzing Loaded Documents\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def diagnose_documents(docs, name):\n",
    "    \"\"\"Analyze documents to see what we have.\"\"\"\n",
    "    print(f\"\\n{name} Documents:\")\n",
    "    print(f\"  Total documents: {len(docs)}\")\n",
    "    \n",
    "    if len(docs) == 0:\n",
    "        print(\"  ❌ No documents loaded!\")\n",
    "        return\n",
    "    \n",
    "    # Check first document\n",
    "    first_doc = docs[0]\n",
    "    print(f\"\\n  First document (PMID: {first_doc['pmid']}):\")\n",
    "    print(f\"    Title: {first_doc['title'][:80]}...\")\n",
    "    print(f\"    Text length: {len(first_doc['text'])} chars\")\n",
    "    print(f\"    Entities: {len(first_doc['entities'])}\")\n",
    "    print(f\"    Relations: {len(first_doc['relations'])}\")\n",
    "    \n",
    "    # Count entity types\n",
    "    entity_types = {}\n",
    "    total_entities = 0\n",
    "    docs_with_entities = 0\n",
    "    \n",
    "    for doc in docs:\n",
    "        if len(doc['entities']) > 0:\n",
    "            docs_with_entities += 1\n",
    "        for ent in doc['entities']:\n",
    "            ent_type = ent['type']\n",
    "            entity_types[ent_type] = entity_types.get(ent_type, 0) + 1\n",
    "            total_entities += 1\n",
    "    \n",
    "    \n",
    "    print(f\"\\n  Entity Statistics:\")\n",
    "    print(f\"    Documents with entities: {docs_with_entities}/{len(docs)}\")\n",
    "    print(f\"    Total entities: {total_entities}\")\n",
    "    print(f\"    Entity types:\")\n",
    "    for ent_type, count in sorted(entity_types.items()):\n",
    "        print(f\"      {ent_type}: {count}\")\n",
    "    \n",
    "    # Check for valid pairs\n",
    "    disease = entity_types.get('Disease', 0)\n",
    "    gene = entity_types.get('Gene', 0) + entity_types.get('GeneOrGeneProduct', 0)\n",
    "    variant = entity_types.get('Variant', 0) + entity_types.get('SequenceVariant', 0)\n",
    "    chemical = entity_types.get('Chemical', 0) + entity_types.get('ChemicalEntity', 0)\n",
    "    \n",
    "    print(f\"\\n  Potential for RE pairs:\")\n",
    "    print(f\"    Disease: {disease}\")\n",
    "    print(f\"    Gene: {gene}\")\n",
    "    print(f\"    Variant: {variant}\")\n",
    "    print(f\"    Chemical: {chemical}\")\n",
    "    print(f\"    Disease-Gene pairs possible: {disease * gene}\")\n",
    "    print(f\"    Disease-Variant pairs possible: {disease * variant}\")\n",
    "    print(f\"    Gene-Variant pairs possible: {gene * variant}\")\n",
    "    \n",
    "    # Show sample entities from first doc\n",
    "    if len(first_doc['entities']) > 0:\n",
    "        print(f\"\\n  Sample entities from first doc:\")\n",
    "        for ent in first_doc['entities'][:5]:\n",
    "            print(f\"    - {ent['type']}: '{ent['text']}' (pos: {ent['start']}-{ent['end']}, id: {ent['id']})\")\n",
    "    \n",
    "    # Show sample relations\n",
    "    if len(first_doc['relations']) > 0:\n",
    "        print(f\"\\n  Sample relations from first doc:\")\n",
    "        for rel in first_doc['relations'][:3]:\n",
    "            print(f\"    - {rel['type']}: {rel['arg1']} ↔ {rel['arg2']}\")\n",
    "    \n",
    "    return entity_types\n",
    "\n",
    "# Diagnose all datasets\n",
    "train_entity_types = diagnose_documents(train_docs, \"TRAIN\")\n",
    "dev_entity_types = diagnose_documents(dev_docs, \"DEV\")\n",
    "test_entity_types = diagnose_documents(test_docs, \"TEST\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# INITIALIZE TOKENIZER\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nInitializing BioBERT tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"✓ Tokenizer loaded with {len(tokenizer)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONVERT TO BERT-GT FORMAT\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nConverting data to BERT-GT format...\\n\")\n",
    "\n",
    "converter = BioREDToBERTGTConverter(tokenizer, max_seq_length=MAX_LENGTH)\n",
    "\n",
    "train_examples = converter.convert_documents(train_docs)\n",
    "dev_examples = converter.convert_documents(dev_docs)\n",
    "test_examples = converter.convert_documents(test_docs)\n",
    "\n",
    "print(f\"\\n✓ Converted:\")\n",
    "print(f\"  Train: {len(train_examples)} examples\")\n",
    "print(f\"  Dev: {len(dev_examples)} examples\")\n",
    "print(f\"  Test: {len(test_examples)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CREATE DATASETS AND DATALOADERS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nCreating datasets...\")\n",
    "\n",
    "train_dataset = BERTGTDataset(train_examples, RELATION_TYPES)\n",
    "dev_dataset = BERTGTDataset(dev_examples, RELATION_TYPES)\n",
    "test_dataset = BERTGTDataset(test_examples, RELATION_TYPES)\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Train: {len(train_dataset)} examples\")\n",
    "print(f\"  Dev: {len(dev_dataset)} examples\")\n",
    "print(f\"  Test: {len(test_dataset)} examples\")\n",
    "print(f\"  Relation types: {train_dataset.relation_types}\")\n",
    "\n",
    "# Create DataLoaders\n",
    "print(\"\\nCreating DataLoaders...\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=bert_gt_collate_fn\n",
    ")\n",
    "\n",
    "dev_loader = DataLoader(\n",
    "    dev_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=bert_gt_collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=bert_gt_collate_fn\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ DataLoaders created:\")\n",
    "print(f\"  Train: {len(train_loader)} batches\")\n",
    "print(f\"  Dev: {len(dev_loader)} batches\")\n",
    "print(f\"  Test: {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Initialize and Train BERT-GT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# INITIALIZE BERT-GT MODEL\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nInitializing BERT-GT model...\")\n",
    "\n",
    "model = BERTGTModel(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_relations=len(RELATION_TYPES),\n",
    "    num_graph_layers=NUM_GRAPH_LAYERS,\n",
    "    num_attention_heads=NUM_ATTENTION_HEADS,\n",
    "    max_entities=MAX_ENTITIES,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n✓ BERT-GT Model initialized:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Hidden size: {model.hidden_size}\")\n",
    "print(f\"  Graph layers: {NUM_GRAPH_LAYERS}\")\n",
    "print(f\"  Attention heads: {NUM_ATTENTION_HEADS}\")\n",
    "print(f\"  Max entities: {MAX_ENTITIES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TRAIN BERT-GT MODEL\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING BERT-GT MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "training_stats = train_bert_gt(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=dev_loader,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LOAD BEST MODEL AND EVALUATE\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nLoading best model...\")\n",
    "model.load_state_dict(torch.load('best_bert_gt_model.pt'))\n",
    "print(\"✓ Best model loaded\")\n",
    "\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_metrics = evaluate_bert_gt(model, test_loader, device)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Test F1 Score: {test_metrics['f1']:.4f}\")\n",
    "print(f\"Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Convert to DataFrame\n",
    "stats_df = pd.DataFrame(training_stats)\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training loss\n",
    "ax1.plot(stats_df['epoch'], stats_df['train_loss'], marker='o', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Training Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation metrics\n",
    "ax2.plot(stats_df['epoch'], stats_df['val_f1'], marker='o', linewidth=2, label='F1 Score')\n",
    "ax2.plot(stats_df['epoch'], stats_df['val_accuracy'], marker='s', linewidth=2, label='Accuracy')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Score', fontsize=12)\n",
    "ax2.set_title('Validation Metrics Over Time', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('bert_gt_training_progress.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Training progress visualized\")\n",
    "print(\"  Saved as: bert_gt_training_progress.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Training Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save statistics\n",
    "stats_df.to_csv('bert_gt_training_stats.csv', index=False)\n",
    "print(\"\\n✓ Training statistics saved to: bert_gt_training_stats.csv\")\n",
    "\n",
    "# Display final stats\n",
    "print(\"\\nTraining Statistics:\")\n",
    "print(stats_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Summary\n",
    "\n",
    "Summary of BERT-GT model performance and comparison with baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BERT-GT MODEL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(f\"  Base Model: BioBERT v1.1\")\n",
    "print(f\"  Graph Transformer Layers: {NUM_GRAPH_LAYERS}\")\n",
    "print(f\"  Attention Heads: {NUM_ATTENTION_HEADS}\")\n",
    "print(f\"  Max Entities per Document: {MAX_ENTITIES}\")\n",
    "print(f\"  Total Parameters: {total_params:,}\")\n",
    "\n",
    "print(\"\\nTraining Configuration:\")\n",
    "print(f\"  Training Documents: {len(train_docs)}\")\n",
    "print(f\"  Training Examples: {len(train_dataset)}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(f\"  F1 Score: {test_metrics['f1']:.4f}\")\n",
    "print(f\"  Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nExpected Performance (Full Dataset):\")\n",
    "print(\"  Entity Pair F1: ~73%\")\n",
    "print(\"  + Relation Type F1: ~59%\")\n",
    "\n",
    "print(\"\\nModel Files:\")\n",
    "print(\"  Best Model: best_bert_gt_model.pt\")\n",
    "print(\"  Training Stats: bert_gt_training_stats.csv\")\n",
    "print(\"  Training Plot: bert_gt_training_progress.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
